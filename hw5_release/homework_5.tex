%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Template file SP 2024
%% Include in directory homework.sty and headerfooter.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}
\usepackage{homework}

\graphicspath{{images/}}
\geometry{letterpaper, portrait, includeheadfoot=true, hmargin=1in, vmargin=1in}

\setcounter{section}{-1}
%% Solution hiding %%
\usepackage[utf8]{inputenc}
\usepackage{lipsum}


\begin{document}
\singlespacing

\renewcommand{\familydefault}{\rmdefault}
\input{headerfooter}

\section{Instructions}

Homework is due Tuesday, April 16, 2024 at 23:59pm Central Time.
Please refer to \url{https://courses.grainger.illinois.edu/cs446/sp2024/homework/hw/index.html} for course policy on homeworks and submission instructions.

\section{GAN: 5pts}
\begin{enumerate}
    \item The problem will be:
    \[\max_{\mathcal{D}}\mathbb{E}_{x \sim p_r(x)}[\log\mathcal{D}(x)] + \mathbb{E}_{x \sim p_g(x)}[\log(1-\mathcal{D}(x))] \]
    Hence, under the given $x$, the optimal choice of $\mathcal{D}(x)$ is:
    \[\mathcal{D}(x) = \frac{p_r(x)}{p_r(x) + p_g(x)}\]

    \item Plugged in the optimal $\mathcal{D}(x)$, Eq. 1 will turn into:
    \[\min_{\mathcal{G}}\mathbb{E}_{x \sim p_r(x)}\left[\log\frac{p_r(x)}{p_r(x) + p_g(x)}\right] + \mathbb{E}_{x \sim p_g(x)}\left[\log\frac{p_g(x)}{p_r(x) + p_g(x)}\right]\]

    \item When $\mathcal{D}$ perfectly classifies generated samples, the output of $\mathcal{D}$ will saturate and the gradient of $\mathcal{D}$ will be almost 0, which makes the gradient of $\mathcal{G}$ almost 0 as well.
\end{enumerate}
\newpage

\section{Diffusion model: 11pts}

\newpage

\section{Unsupervised learning / contrastive learning: 4 pts}
\begin{enumerate}
    \item True.
    \item False. MAE is an approach for computer vision.
    \item to be done 
    \item False. CLIP enables zero-shot classification with contrastive pre-training.
\end{enumerate}
\newpage

\section{Coding: GAN, 10pts}

\newpage

\section{Coding: Diffusion model, 10pts}

\end{document}
