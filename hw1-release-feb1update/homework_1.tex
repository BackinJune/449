%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Template file SP 2024
%% Include in directory homework.sty and headerfooter.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}
\usepackage{homework}

\graphicspath{{images/}}
\geometry{letterpaper, portrait, includeheadfoot=true, hmargin=1in, vmargin=1in}

\setcounter{section}{-1}
%% Solution hiding %%
\usepackage[utf8]{inputenc}
\usepackage{lipsum}


\begin{document}
\singlespacing
\textbf{Bingjun Guo (bingjun3)}

\renewcommand{\familydefault}{\rmdefault}
\input{headerfooter}

\section{Instructions}
Homework is due Thursday, February 6, 2024 at 23:59pm Central Time.
Please refer to \url{https://courses.grainger.illinois.edu/cs446/sp2024/homework/hw/index.html} for course policy on homeworks and submission instructions.

% 1
\section{Short answer: 10pts}
\begin{enumerate}
\item $O(MNd)$
\item $k=10$
\item $
    \left(\begin{bmatrix}
        1 \\ 1
        \end{bmatrix}, 1
    \right)$
\item The largest eigenvalue of $A^\top A$ is the square of the largest singular value of $A$.
\item In sentiment recognization tasks on natural language, for example, movie comments, for 
the probability of the phrase ``not good'' appearing in a positive comment:
\[P(``not", ``good"|positive) \neq P(``not"|positive) \cdot P(``good"|positive)\]
since both $P(``not"|positive)$ and $P(``good"|negative)$ are adequately high but 
``not good'' should appear really rare in positive comments.
\end{enumerate}

% 2
\section{Linear Regression: 10pts}
\begin{enumerate}
    \item $X$ can be considered as a linear transform from $\mathbb{R}^n$ to 
    $\mathbb{R}^d$. Thus, it complies to the Rank-Nullity Theorem: 
    \[\text{rank}(X) + \text{nullity}(X) = n\]
    in which the dimention of the input space is $n$ and $\text{rank}(X) = n$.\\
    Therefore, $\text{nullity}(X) = 0$, which indicates that $X$ is invertible. Thus,
    there exists\\ $\textbf{\textit{w}} = X^{-1}\textbf{\textit{y}}$ such that satisfies
    $X\textbf{\textit{w}} = \textbf{\textit{y}}$.
    \item Since the number of non-zero singular values of $A$ equals to $\text{rank}(A)$, 
    $\Sigma$ is a diagonal matrix consists of positive singular values of $A$, and $X$
    is real, $\text{rank}(\Sigma) = \text{rank}(A) = n$.
    \item Firstly we will prove that $X^{\top}$ and $XX^{\top}$ share the same nullity,\\
    \textit{i.e.}, $X^{\top}M = 0 \iff XX^{\top}M = 0$ for $M \in \mathbb{R}^n$.\\
    $X \cdot 0 = 0$, thus $X^{\top}M = 0 \rightarrow X(X^{\top}M) = 0 \rightarrow XX^{\top}M = 0$.
    \\Supoose $XX^{\top}M = 0$, then we have $M^{\top}XX^{\top}M = 0$, and thus
    $(X^{\top}M)^{\top}X^{\top}M = 0$, with $X^{\top}M \in \mathbb{R}^d$.
    $(X^{\top}M)^{\top}X^{\top}M$ equals to sum of square of all the entries in $X^{\top}M$,
    which can only be greater or equal to 0 since its a real vector. Thus, all entries in
    $X^{\top}M$ are 0, \textit{i.e.}, $X^{\top}M = \bf{0}$. Therefore, 
    $XX^{\top}M = 0 \rightarrow X^{\top}M = 0$.\\
    Secondly, since $\text{nullity}(X^{\top}) = \text{nullity}(XX^{\top})$, according to the 
    Rank-Nullity Theorem introduced in the first question, since the dimention of input space
    (RHS of the equation) is both $n$ for linear transforms
    $X^{\top}:\mathbb{R}^n \rightarrow \mathbb{R}^d$ and 
    $XX^{\top}: \mathbb{R}^n \rightarrow \mathbb{R}^n$, 
    $\text{rank}(XX^{\top}) = \text{rank}(X^{\top}) = \text{rank}(X) = n$. 
    Thus, $XX^{\top}$ is a full-rank square matrix, \textit{i.e.}, $XX^{\top}$ is invertible.
\end{enumerate}

%3
\section{SVM: 10 pts}
\begin{enumerate}
    \item 2, which happens in the case that the closest two vectorsa in different class are
    selected and there's only such 2 points in $\mathcal{D}$ belonging to different classes that have such distance between each other.
    \item The largest possible \bf{TO BE DONE}
    \item (a) $\phi(\vx) = (x_1^2, x_2^2, \sqrt2x_1x_2, \sqrt2x_1, \sqrt2x_2, 1)$\\
    (b) \\
    $\phi(-1, -1) = (1, 1, \sqrt2, -\sqrt2, -\sqrt2, 1)$\\
    $\phi(1, 1) = (1, 1, \sqrt2, \sqrt2, \sqrt2, \sqrt2, 1)$\\
    $\phi(1, -1) = (1, 1, -\sqrt2, \sqrt2, -\sqrt2, 1)$\\
    $\phi(-1, 1) = (1, 1, -\sqrt2, -\sqrt2, \sqrt2, 1)$\\
    Therefore, $\vw$ can be $(0, 0, 1, 0, 0, 0)$.
\end{enumerate}

%4
\section{Gaussian Naive Bayes: 15pts}
\begin{enumerate}
    \item 
    \[\frac{1}{1+\text{exp}(\log\frac{A}{B})} = \frac{B}{B+A}\]
    \[P(y=+1|\vx) = \frac{P(\vx|y=+1) \cdot p}{P(\vx)} = \frac{P(\vx|y=+1) \cdot p}{P(\vx|y=+1) \cdot p + P(\vx|y=-1) \cdot (1-p)}\]
    Therefore, for $B = P(\vx|y=+1) \cdot p$ and $A = P(\vx|y=-1) \cdot (1-p)$, 
    \[P(y=+1|\vx) = \frac{1}{1+\text{exp}(\log\frac{A}{B})}\]
    \item \bf{TO BE DONE}
    \item \[P(y|\vx) = \frac{1}{1 + \text{exp}(y\cdot(\vw^{\top}\vx+b))}\]
\end{enumerate}

%5
\section{Linear regression: 14pts + 1pt}

\end{document}