%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Template file SP 2024
%% Include in directory homework.sty and headerfooter.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}
\usepackage{homework}

\graphicspath{{images/}}
\geometry{letterpaper, portrait, includeheadfoot=true, hmargin=1in, vmargin=1in}

\setcounter{section}{-1}
%% Solution hiding %%
\usepackage[utf8]{inputenc}
\usepackage{lipsum}

\begin{document}
\singlespacing

\renewcommand{\familydefault}{\rmdefault}
\input{headerfooter}

\section{Instructions}

Homework is due Tuesday, February 20, 2024 at 23:59pm Central Time.
Please refer to \url{https://courses.grainger.illinois.edu/cs446/sp2024/homework/hw/index.html} for course policy on homeworks and submission instructions.

\section{Soft-margin SVM: 4pts}
Referring to hard-margin case, soft-margin SVM can be simplified in such form:
\[\min_{\boldsymbol{w} \in \mathbb{R}^d,\boldsymbol{\xi} \in \mathbb{R}^n_{\geq 0}} 
\max_{\boldsymbol{\alpha} \in \mathbb{R}^n_+, \boldsymbol{\beta} \in \mathbb{R}^n_+} 
\sum_{i \in [n]} \alpha_i (1 - \xi_i - y^{(i)} \boldsymbol{w}^{\top} \boldsymbol{x}^{(i)}) 
+ \sum_{i \in [n]} -\beta_i \xi_i
+ \frac{\lambda}{2}\|\boldsymbol{w}\|_2^2
+ C \sum_{i \in [n]}\xi_i\]
with constraints:
\[y^{(i)} \boldsymbol{w}^{\top} \boldsymbol{x}^{(i)} \geq 1 - \xi_i
,\quad
\xi_i \geq 0\]
and thus we have the dual problem:
\[D(\boldsymbol{\alpha}) = 
\min_{\boldsymbol{w} \in \mathbb{R}^d,\boldsymbol{\xi} \in \mathbb{R}^n_{\geq 0}} 
\sum_{i \in [n]} \alpha_i (1 - \xi_i - y^{(i)} \boldsymbol{w}^{\top} \boldsymbol{x}^{(i)})
+ \sum_{i \in [n]} -\beta_i \xi_i
+ \frac{\lambda}{2}\|\boldsymbol{w}\|_2^2
+ C \sum_{i \in [n]}\xi_i\]
For the gradient of the problem with respect to $\boldsymbol{w}$:
\[\nabla_{\boldsymbol{w}}
\sum_{i \in [n]} \alpha_i (1 - \xi_i - y^{(i)} \boldsymbol{w}^{\top} \boldsymbol{x}^{(i)})
+ \sum_{i \in [n]} -\beta_i \xi_i
+ \frac{\lambda}{2}\|\boldsymbol{w}\|_2^2
+ C \sum_{i \in [n]}\xi_i
= 0\]
\[\Rightarrow \boldsymbol{w} = \sum_{i \in [n]} \alpha_i y^{(i)} x^{(i)}\]
For the gradient of the problem with respect to $\boldsymbol{\xi}$:
\[\nabla_{\boldsymbol{\xi}}
\sum_{i \in [n]} \alpha_i (1 - \xi_i - y^{(i)} \boldsymbol{w}^{\top} \boldsymbol{x}^{(i)})
+ \sum_{i \in [n]} -\beta_i \xi_i
+ \frac{\lambda}{2}\|\boldsymbol{w}\|_2^2
+ C \sum_{i \in [n]}\xi_i
= 0\]
\[\Rightarrow \beta_i = C - \alpha_i\]
Therefore, the dual form of soft-margin SVM will be:
\[\max_{\alpha_{i,j} \in (0, C)}
\sum_{i \in [n]} \alpha_i - 
\frac{1}{2}\sum_{i,j \in [n]}\alpha_i\alpha_j y^{(i)} y^{(j)} 
\boldsymbol{x}^{(i)\top} \boldsymbol{x}^{(j)}\]
\newpage

\section{SVM, RBF Kernel and Nearest Neighbor: 6pts}
\begin{enumerate}
    \item \[\hat{\boldsymbol{w}} = \sum_{i \in [n]} \hat{\alpha_i}y^{(i)}\boldsymbol{x}^{(i)}\]
    \[\Rightarrow f(\boldsymbol{x}) = \hat{\boldsymbol{w}}^{\top}\boldsymbol{x}
    = \sum_{i \in [n]} \hat{\alpha_i}y_i\boldsymbol{x}_i^{\top}\boldsymbol{x}\]
    \item \[f_\sigma(\boldsymbol{x}) = \hat{\boldsymbol{w}}^{\top}\kappa(\boldsymbol{x}_i, \boldsymbol{x}_j)
    = \sum_{i \in [n]} \hat{\alpha_i}y_i \exp \left( -\frac{\|\boldsymbol{x}_i - \boldsymbol{x}_j\|_2^2}{2\sigma^2} \right)\]
    \item \[f_\sigma(\boldsymbol{x}) = \hat{\boldsymbol{w}}^{\top}\kappa(\boldsymbol{x}_i, \boldsymbol{x}_j)
    = \sum_{i \in S} \hat{\alpha_i}y_i \exp \left( -\frac{\|\boldsymbol{x}_i - \boldsymbol{x}_j\|_2^2}{2\sigma^2} \right)\]
    \[= \sum_{i \in T} \hat{\alpha_i}y_i \exp \left( -\frac{\|\boldsymbol{x}_i - \boldsymbol{x}_j\|_2^2}{2\sigma^2} \right)
    + \sum_{i \in S\backslash T} \hat{\alpha_i}y_i \exp \left( -\frac{\|\boldsymbol{x}_i - \boldsymbol{x}_j\|_2^2}{2\sigma^2} \right)\]
    \[= \sum_{i \in T} \hat{\alpha_i}y_i \exp \left( -\frac{\rho^2}{2\sigma^2} \right)
    + \sum_{i \in S\backslash T} \hat{\alpha_i}y_i \exp \left( -\frac{c}{2\sigma^2} \right)\] 
    \[\Longrightarrow \frac{f_\sigma(\boldsymbol{x})}{\exp\left(-\rho^2/2\sigma^2\right)}
    = \sum_{i \in T} \hat{\alpha_i}y_i + \sum_{i \in S\backslash T} \hat{\alpha_i}y_i \exp \left(\frac{\rho^2 - \|\boldsymbol{x}_i - \boldsymbol{x}\|_2^2}{2\sigma^2} \right)\]
    Since:
    \[\forall i \in S\backslash T,\: \rho^2 < \|\boldsymbol{x}_i - \boldsymbol{x}_j\|_2^2\]
    we have the limit:
    \[\lim_{\sigma\rightarrow0}\frac{\rho^2 - \|\boldsymbol{x}_i - \boldsymbol{x}\|_2^2}{2\sigma^2}\rightarrow-\infty\]
    hence:
    \[\lim_{\sigma\rightarrow0} \frac{f_\sigma(\boldsymbol{x})}{\exp\left(-\rho^2/2\sigma^2\right)}
    = \lim_{\sigma\rightarrow0}
    \sum_{i \in T} \hat{\alpha_i}y_i + \sum_{i \in S\backslash T} \hat{\alpha_i}y_i \exp \left(\frac{\rho^2 - \|\boldsymbol{x}_i - \boldsymbol{x}\|_2^2}{2\sigma^2} \right)\] 
    \[= \sum_{i \in T} \hat{\alpha_i}y_i + 0 = \sum_{i \in T} \hat{\alpha_i}y_i\]
\end{enumerate}
\newpage

\section{Decision Tree and Adaboost: 12 pts}
\begin{enumerate}
    \item \[I(\mathcal{D}) = - \sum_{y\in{-1,1}}p(y|\mathcal{D})\log_2 p(y|\mathcal{D})\]
    \[= -p(1|\mathcal{D})\log_2p(1|\mathcal{D}) -p(-1|\mathcal{D})\log_2p(-1|\mathcal{D})\]
    \[= -\frac{1}{2}\log_2\frac{1}{2} - \frac{1}{2}\log_2\frac{1}{2} = 1\]
    \item rule: $f_1:\text{sign}(x_1 - 4.5)$\\
    information gain with 3 green 1 blue v.s. 2 blue:
    \[IG(\mathcal{D},f_1) = I(\mathcal{D}) - \sum_{j=1}^{2}\frac{|\mathcal{D}_j|}{|\mathcal{D}|}I(\mathcal{D}_j)\]
    \[= 1 - \frac{4}{6}I(\mathcal{D}_1) - \frac{2}{6}I(\mathcal{D}_2)
    = 1 - \frac{2}{3}(-\frac{3}{4}\log_2\frac{3}{4} - \frac{1}{4}\log_2\frac{1}{4}) - \frac{1}{3}(-\log_2 1)\]
    \[= \frac{1}{2}\log_2 3 - \frac{1}{3}\]
    \item rule (for the left node holding $\mathcal{D}_1$):
    $f_2: \text{sign}(1.5 - x_2)$\\
    In the left node, information gain with 3 green v.s. 1 blue:
    \[IG(\mathcal{D}_1,f_2) = I(\mathcal{D}_1) - I(\mathcal{D}_1|f_2)\] 
    Since all samples are in perfect split in $\mathcal{D}_1$ with $f_2$, we have:
    $I(\mathcal{D}_1|f_2) = 0$. Thus,
    \[IG(\mathcal{D}_1, f_2) = I(\mathcal{D}_1) = -\frac{3}{4}\log_2\frac{3}{4} - \frac{1}{4}\log_2\frac{1}{4} = \frac{3}{4}\log_2 3 - 2\]
    For the right node holding $\mathcal{D}_2$, since the first split has already been perfect for all samples, there's no need for the second split and thus the information gain of the second split equals to 0.
    \item For iteration $t = 1$, $f_1:x_j\geq4.5$:
    \[\gamma_1 = [\frac{1}{6}, \frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6},\frac{1}{6}]^{\top}\]
    \[\epsilon_1 = \frac{\sum_{i:y^{(i)}\neq f_1(\boldsymbol{x}^{(i)})}\gamma_i^1}{\sum_i\gamma_i^1} = \frac{1}{6}\]
    \[z_1 = \sum_{i=1}^6\gamma_1^{(i)}y^{(i)}f_1(\boldsymbol{x}^{(i)}) = \frac{5}{6} - \frac{1}{6} = \frac{2}{3}\]
    \[\alpha_1 = \frac{1}{2}\ln\frac{1+z_1}{1-z_1} = \frac{1}{2}\ln5\]
    \[\gamma_2^{(i)} = \frac{\gamma_1^{(i)}\exp(-\alpha_1y^{(i)}f_t(\boldsymbol{x}^{(i)}))}{Z_t}\]
    with $\gamma_1^{(i)}\exp(-\alpha_1y^{(i)}f_t(\boldsymbol{x}^{(i)})) = 5^{-\frac{1}{2}}$ for $i = 1, 3, 4, 5, 6$ and $5^{\frac{1}{2}}$ for $i = 2$, while\:$5^{\frac{1}{2}} = 5\cdot5^{-\frac{1}{2}}$. Thus,
    \[\gamma_2 = [\frac{1}{10}, \frac{1}{2}, \frac{1}{10}, \frac{1}{10}, \frac{1}{10}, \frac{1}{10}]^{\top}\]
    For iteration $t=2$, $f_2:x_j<1.5$:
    \[\epsilon_2 = \frac{\sum_{i:y^{(i)}\neq f_2(\boldsymbol{x}^{(i)})}\gamma_i^2}{\sum_i\gamma_i^2} = \frac{2\cdot\frac{1}{10}}{1} = \frac{1}{5}\]
    \[z_2 = \sum_{i=1}^6\gamma_2^{(i)}y^{(i)}f_2(\boldsymbol{x}^{(i)}) = \frac{1}{2} + 3\cdot\frac{1}{10} - 2\cdot\frac{1}{10} = \frac{3}{5}\]
    \[\alpha_2 = \frac{1}{2}\ln\frac{1+z_2}{1-z_2} = \frac{1}{2}\ln4\]
    \item Final classifier:
    \[f(\boldsymbol{x}) = \text{sign}(\alpha_1f_1(\boldsymbol{x}) + \alpha_2f_2(\boldsymbol{x}))\]
\end{enumerate}


\newpage

\section{Learning Theory: 14pts}

\newpage

\section{Coding: SVM, 4pts}
% Include your plot for Q5.3.

\end{document}